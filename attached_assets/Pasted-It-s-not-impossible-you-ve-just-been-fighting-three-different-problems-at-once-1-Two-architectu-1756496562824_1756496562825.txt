It’s not impossible — you’ve just been fighting three different problems at once:
	1.	Two architectures mixed together.
Locally (Replit preview) you had a stateful Express server. On Vercel you have static hosting + serverless. Posting a giant league JSON to your own /api/* worked locally, but fails on Vercel (404 HTML pages, payload limits, timeouts).
	2.	Wrong endpoints on Vercel.
The client still called /api/process-league and friends, but those don’t exist as serverless functions. Vercel returns an HTML 404 (“The page could not be found”), and your code tries to JSON.parse that → boom.
	3.	Big-file parsing on the main thread.
Even when the data arrives, doing gunzip + JSON.parse on the main UI thread can freeze the tab and look like “infinite loading”.

The fix really does work (it’s what BBGM does): keep all import work in the browser and use one tiny proxy only for downloading remote URLs. If something still breaks, it’s almost always one of these: (a) the proxy file isn’t where Vercel expects it, (b) the client still calls old /api/* endpoints, or (c) the Web Worker isn’t loading. Here’s a short, fool-proof checklist you can hand to Replit to make this bulletproof — unified method, same in preview and Vercel.

⸻

10-Minute Fix — Unified, BBGM-style (copy to Replit)

0) Install once

npm i fflate

1) Vercel serverless proxy (URL → bytes, nothing else)

Create at repo root: api/fetch-league.ts

import type { VercelRequest, VercelResponse } from "@vercel/node";
import { Readable } from "node:stream";

export const config = { maxDuration: 60 };

const UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36";

function normalize(input: string) {
  const u = new URL(input.trim());
  if (u.hostname.endsWith("dropbox.com")) { u.hostname = "dl.dropboxusercontent.com"; u.searchParams.set("dl","1"); }
  if (u.hostname === "github.com") {
    const p = u.pathname.split("/").filter(Boolean);
    if (p.length >= 5 && p[2] === "blob") {
      const [user, repo, _blob, branch, ...rest] = p;
      u.hostname = "raw.githubusercontent.com";
      u.pathname = `/${user}/${repo}/${branch}/${rest.join("/")}`;
      u.search = "";
    }
  }
  if (u.hostname === "gist.github.com") {
    const p = u.pathname.split("/").filter(Boolean);
    if (p.length >= 2) { const [user, hash] = p; u.hostname = "gist.githubusercontent.com"; u.pathname = `/${user}/${hash}/raw`; u.search = ""; }
  }
  if (u.hostname === "drive.google.com" && u.pathname.startsWith("/file/")) {
    const id = u.pathname.split("/")[3];
    u.pathname = "/uc"; u.search = ""; u.searchParams.set("export","download"); u.searchParams.set("id", id);
  }
  if (!/^https?:$/.test(u.protocol)) throw new Error("Only http(s) URLs are allowed.");
  return u.toString();
}

export default async function handler(req: VercelRequest, res: VercelResponse) {
  try {
    const url = typeof req.query.url === "string" ? req.query.url : "";
    if (!url) return res.status(400).json({ error: "Missing ?url=" });

    const normalized = normalize(url);
    const looksGzip = /\.json\.gz$|\.gz$/i.test(new URL(normalized).pathname);

    const upstream = await fetch(normalized, {
      redirect: "follow",
      headers: { "User-Agent": UA, Accept: "*/*", "Accept-Encoding": "identity" }
    });

    if (!upstream.ok || !upstream.body) {
      return res.status(upstream.status || 502).json({ error: `Fetch failed: remote ${upstream.status} ${upstream.statusText}` });
    }

    const ct = upstream.headers.get("content-type") || "application/octet-stream";
    res.setHeader("Content-Type", ct);

    const ce = upstream.headers.get("content-encoding");
    const isGzipType = /\b(gzip|x-gzip)\b/i.test(ct) || /application\/(gzip|x-gzip)/i.test(ct);
    if (ce) res.setHeader("X-Content-Encoding", ce);
    else if (looksGzip || isGzipType) res.setHeader("X-Content-Encoding", "gzip");

    res.setHeader("Cache-Control", "no-store");
    Readable.fromWeb(upstream.body as any).pipe(res);
  } catch (e: any) {
    res.status(400).json({ error: String(e?.message || e) });
  }
}

Create vercel.json (repo root):

{
  "functions": { "api/**": { "maxDuration": 60 } }
}

Vercel Project Settings:
	•	Framework preset: Vite
	•	Build command: npm run build
	•	Output directory: dist/public

This function MUST be exactly at /api/fetch-league.ts in the repo root, not inside server/ or src/.

2) Dev parity (so preview uses same path)

Add an Express GET route /api/fetch-league with the same logic in your dev router (if preview needs it). If preview already works with the serverless function path, skip.

3) Client loader (one codepath)

Create: client/src/lib/leagueIO.ts

import { gunzipSync } from "fflate";

export async function fetchLeagueBytes(url: string) {
  const r = await fetch(`/api/fetch-league?url=${encodeURIComponent(url)}`);
  if (!r.ok) { const t = await r.text().catch(()=>""); throw new Error(`URL fetch failed (${r.status}): ${t || r.statusText}`); }
  const hinted = r.headers.get("x-content-encoding") as ("gzip" | null);
  const bytes = new Uint8Array(await r.arrayBuffer());
  return { bytes, hinted };
}

export async function fileToBytes(file: File) {
  const bytes = new Uint8Array(await file.arrayBuffer());
  const hinted = file.name.toLowerCase().endsWith(".gz") ? ("gzip" as const) : null;
  return { bytes, hinted };
}

// optional small-file sync
export function parseLeagueSync(bytes: Uint8Array, hinted?: "gzip" | null) {
  const needGz = hinted === "gzip" || (bytes[0] === 0x1f && bytes[1] === 0x8b);
  const raw = needGz ? gunzipSync(bytes) : bytes;
  const text = new TextDecoder().decode(raw);
  return JSON.parse(text);
}

Create worker: client/src/workers/leagueParse.worker.ts

import { gunzip } from "fflate";
const isGzip = (u8: Uint8Array) => u8.length >= 2 && u8[0] === 0x1f && u8[1] === 0x8b;

self.onmessage = async (e: MessageEvent) => {
  try {
    const { bytes, hinted } = e.data as { bytes: ArrayBuffer; hinted?: "gzip" | null };
    let u8 = new Uint8Array(bytes); // transferred
    if (hinted === "gzip" || isGzip(u8)) {
      u8 = await new Promise<Uint8Array>((res, rej) => gunzip(u8, (err, out) => err ? rej(err) : res(out)));
    }
    const text = new TextDecoder().decode(u8);
    const league = JSON.parse(text);
    (self as any).postMessage({ ok: true, league });
  } catch (err: any) {
    (self as any).postMessage({ ok: false, error: String(err?.message || err) });
  }
};

Append to leagueIO.ts:

export async function parseLeagueInWorker(bytes: Uint8Array, hinted: "gzip" | null) {
  const ab = bytes.buffer.slice(bytes.byteOffset, bytes.byteOffset + bytes.byteLength);
  const worker = new Worker(new URL("../workers/leagueParse.worker.ts", import.meta.url), { type: "module" });
  return new Promise<any>((resolve, reject) => {
    const done = () => worker.terminate();
    worker.onmessage = (e) => { done(); const { ok, league, error } = e.data || {}; ok ? resolve(league) : reject(new Error(error || "Parse failed")); };
    worker.onerror = (ev) => { done(); reject(new Error(ev.message || "Worker error")); };
    worker.postMessage({ bytes: ab, hinted }, [ab as any]); // transfer
  });
}

TS config tweak (so worker types compile): in tsconfig.json

{
  "compilerOptions": {
    "lib": ["ES2022", "DOM", "WebWorker"],
    "types": []
  }
}

4) Swap just the loader in your existing upload component

Do not change your UI/game. Just replace the “read + parse” bit:

import { fetchLeagueBytes, fileToBytes, parseLeagueInWorker } from "@/lib/leagueIO";

async function onUrlSubmit(url: string) {
  setLoading(true);
  try {
    const { bytes, hinted } = await fetchLeagueBytes(url);
    const league = await parseLeagueInWorker(bytes, hinted);
    // ✅ Use league directly in your existing UI/game code (NO POST to /api/*)
  } catch (e: any) {
    toast.error(e.message || "Failed to load URL");
  } finally { setLoading(false); }
}

async function onFileChosen(file: File) {
  setLoading(true);
  try {
    const { bytes, hinted } = await fileToBytes(file);
    const league = await parseLeagueInWorker(bytes, hinted);
    // ✅ Same: feed league into your existing functions/state
  } catch (e: any) {
    toast.error(e.message || "Failed to load file");
  } finally { setLoading(false); }
}

File input accept:

<input type="file" accept=".json,.gz,application/gzip,application/x-gzip" />

5) Remove old server calls

Search and delete any client calls to other /api/* like:
	•	/api/process-league, /api/generate-grid, /api/new-game, etc.

Only /api/fetch-league should remain.

⸻

Two-minute diagnostics (to prove where it’s failing)
	1.	Is the Vercel function reachable?
Open: https://YOUR-SITE.vercel.app/api/fetch-league?url=https://raw.githubusercontent.com/github/gitignore/main/Node.gitignore
You should see a stream (not an HTML “not found” page). If it 404s, your function path or build/output dir is wrong.
	2.	Is the client still calling old APIs?
In DevTools → Network, try a league import. You should see only /api/fetch-league (for URL). If you see anything else under /api/*, that’s the culprit — delete/replace those calls.
	3.	Is the worker loading?
Add a one-liner before creating it:

console.log("Spawning parse worker…");

If the console shows an error like “Failed to construct ‘Worker’…”, it’s almost always a bundling issue. The new URL("../workers/leagueParse.worker.ts", import.meta.url) form is the Vite-approved way. Also confirm tsconfig.lib includes "WebWorker".

⸻

Why this won’t time out or crash
	•	No giant POSTs to Vercel → no payload limits, no “request entity too large”.
	•	The serverless function only streams bytes from Dropbox/GitHub/Drive — trivial CPU/memory, super reliable.
	•	All decompression + JSON.parse in a Web Worker → UI stays responsive even for huge leagues.
	•	We sniff gzip by header + magic bytes, so both .json and .gz work regardless of host quirks.

⸻

If Replit implements exactly this (file locations and names matter), it will work in preview and on Vercel. If it still fails, the three quick diagnostics above will pinpoint the issue in under a minute.